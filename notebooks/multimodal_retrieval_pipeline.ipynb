{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtVhlr5OwPDJ"
      },
      "source": [
        "# ğŸ” Multimodal Retrieval System\n",
        "## Using Danish's Smart Gallery Models & Pre-computed Embeddings\n",
        "\n",
        "**Models:**\n",
        "- Face: InsightFace buffalo_l (512-dim)\n",
        "- Image: CLIP ViT-L-14 (768-dim)\n",
        "- Detection: Your fine-tuned YOLOv8x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trmssIAJwPDN"
      },
      "source": [
        "## 1ï¸âƒ£ Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWfmaYPCwPDO",
        "outputId": "793dc199-46ac-4d87-de67-18b909aa3039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 17 20:27:06 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   31C    P0             55W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA: True\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka-VZdb3wPDS",
        "outputId": "87107eff-6be6-4f6e-b530-d923a67f62a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/439.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q insightface onnxruntime-gpu open-clip-torch faiss-cpu ultralytics\n",
        "!pip install -q fastapi uvicorn python-multipart pyngrok nest-asyncio\n",
        "print(\"âœ… Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S2_aMOUwPDU",
        "outputId": "4c2305a5-6a63-44df-f1b5-4aee73cdba82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Google Drive mounted!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Google Drive mounted!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-download CIFAR-100 and extract to the same path\n",
        "import os\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create directory\n",
        "os.makedirs(\"/content/datasets/images/cifar100\", exist_ok=True)\n",
        "\n",
        "# Download CIFAR-100\n",
        "print(\"Downloading CIFAR-100...\")\n",
        "dataset = torchvision.datasets.CIFAR100(\n",
        "    root='/content/datasets',\n",
        "    train=True,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Save images with same naming convention\n",
        "print(\"Saving images...\")\n",
        "for idx in tqdm(range(len(dataset))):\n",
        "    img, label = dataset[idx]\n",
        "    filename = f\"/content/datasets/images/cifar100/{idx:05d}_class{label}.png\"\n",
        "    img.save(filename)\n",
        "\n",
        "print(f\"âœ… Saved {len(dataset)} images!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if9ups_OkMZV",
        "outputId": "94436ee1-423d-4286-dfb5-3c7668f5da32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CIFAR-100...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169M/169M [00:05<00:00, 29.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:18<00:00, 2773.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved 50000 images!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml9uhamXwPDV"
      },
      "source": [
        "## 2ï¸âƒ£ Configure Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9jJ9tnXwPDX",
        "outputId": "9fef0e25-3d18-473d-b0ed-142b69f52b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking paths...\n",
            "  âœ… Face embeddings\n",
            "  âœ… CLIP embeddings\n",
            "  âœ… Image metadata\n",
            "  âœ… Image paths CSV\n",
            "  âœ… FAISS index\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# =============================================================================\n",
        "# UPDATE THESE PATHS TO MATCH YOUR GOOGLE DRIVE\n",
        "# =============================================================================\n",
        "ML_OUTPUTS_BASE = \"/content/drive/MyDrive/ml_pipeline_outputs\"\n",
        "\n",
        "# Face embeddings\n",
        "FACE_EMBEDDINGS_PATH = f\"{ML_OUTPUTS_BASE}/face_embeddings/face_embeddings.npy\"\n",
        "FACE_METADATA_PATH = f\"{ML_OUTPUTS_BASE}/face_embeddings/identity_mapping.json\"\n",
        "\n",
        "# Image embeddings\n",
        "IMAGE_EMBEDDINGS_DIR = f\"{ML_OUTPUTS_BASE}/face_embeddings/image_embeddings\"\n",
        "CLIP_EMBEDDINGS_PATH = f\"{IMAGE_EMBEDDINGS_DIR}/image_embeddings_clip.npy\"\n",
        "IMAGE_METADATA_PATH = f\"{IMAGE_EMBEDDINGS_DIR}/image_metadata.json\"\n",
        "IMAGE_PATHS_CSV = f\"{IMAGE_EMBEDDINGS_DIR}/image_paths.csv\"\n",
        "FAISS_CLIP_PATH = f\"{IMAGE_EMBEDDINGS_DIR}/faiss_clip.index\"\n",
        "\n",
        "# YOLO model\n",
        "YOLO_MODEL_PATH = f\"{ML_OUTPUTS_BASE}/yolo_training/yolov8x_coco/weights/best.pt\"\n",
        "\n",
        "# Verify paths\n",
        "print(\"Checking paths...\")\n",
        "for name, path in [\n",
        "    (\"Face embeddings\", FACE_EMBEDDINGS_PATH),\n",
        "    (\"CLIP embeddings\", CLIP_EMBEDDINGS_PATH),\n",
        "    (\"Image metadata\", IMAGE_METADATA_PATH),\n",
        "    (\"Image paths CSV\", IMAGE_PATHS_CSV),\n",
        "    (\"FAISS index\", FAISS_CLIP_PATH),\n",
        "]:\n",
        "    status = \"âœ…\" if os.path.exists(path) else \"âŒ\"\n",
        "    print(f\"  {status} {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UvORH5bwPDZ"
      },
      "source": [
        "## 3ï¸âƒ£ Load Embeddings & Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtJGlOfowPDa",
        "outputId": "dbb7b3be-ddd6-4e57-ebc6-db6370318fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings...\n",
            "  âœ… Face: (9962, 512)\n",
            "  âœ… CLIP: (4998, 768)\n",
            "  âœ… Metadata: 11 entries\n",
            "  âœ… Image paths: 4998 files\n",
            "  âœ… Face metadata: 9962 entries\n",
            "\n",
            "Loading FAISS index...\n",
            "  âœ… Loaded pre-built index: 4998 vectors\n",
            "  âœ… Face index: 9962 vectors\n",
            "\n",
            "ğŸ“Š Total: 4998 images, 9962 faces\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import faiss\n",
        "\n",
        "# Load face embeddings\n",
        "print(\"Loading embeddings...\")\n",
        "face_embeddings = np.load(FACE_EMBEDDINGS_PATH) if os.path.exists(FACE_EMBEDDINGS_PATH) else None\n",
        "if face_embeddings is not None:\n",
        "    print(f\"  âœ… Face: {face_embeddings.shape}\")\n",
        "\n",
        "# Load CLIP embeddings\n",
        "clip_embeddings = np.load(CLIP_EMBEDDINGS_PATH) if os.path.exists(CLIP_EMBEDDINGS_PATH) else None\n",
        "if clip_embeddings is not None:\n",
        "    print(f\"  âœ… CLIP: {clip_embeddings.shape}\")\n",
        "\n",
        "# Load image metadata (JSON)\n",
        "image_metadata = {}\n",
        "if os.path.exists(IMAGE_METADATA_PATH):\n",
        "    with open(IMAGE_METADATA_PATH, 'r') as f:\n",
        "        image_metadata = json.load(f)\n",
        "    print(f\"  âœ… Metadata: {len(image_metadata)} entries\")\n",
        "\n",
        "# Load image paths (CSV) - this has the actual file paths\n",
        "image_paths_list = []\n",
        "if os.path.exists(IMAGE_PATHS_CSV):\n",
        "    df = pd.read_csv(IMAGE_PATHS_CSV)\n",
        "    image_paths_list = df['path'].tolist() if 'path' in df.columns else df.iloc[:, 0].tolist()\n",
        "    print(f\"  âœ… Image paths: {len(image_paths_list)} files\")\n",
        "\n",
        "# Load face metadata\n",
        "face_metadata = {}\n",
        "if os.path.exists(FACE_METADATA_PATH):\n",
        "    with open(FACE_METADATA_PATH, 'r') as f:\n",
        "        face_metadata = json.load(f)\n",
        "    print(f\"  âœ… Face metadata: {len(face_metadata)} entries\")\n",
        "\n",
        "# Load or build FAISS index\n",
        "print(\"\\nLoading FAISS index...\")\n",
        "if os.path.exists(FAISS_CLIP_PATH):\n",
        "    image_index = faiss.read_index(FAISS_CLIP_PATH)\n",
        "    print(f\"  âœ… Loaded pre-built index: {image_index.ntotal} vectors\")\n",
        "elif clip_embeddings is not None:\n",
        "    image_index = faiss.IndexFlatIP(768)\n",
        "    clip_norm = clip_embeddings / np.linalg.norm(clip_embeddings, axis=1, keepdims=True)\n",
        "    image_index.add(clip_norm.astype(np.float32))\n",
        "    print(f\"  âœ… Built new index: {image_index.ntotal} vectors\")\n",
        "else:\n",
        "    image_index = faiss.IndexFlatIP(768)\n",
        "\n",
        "# Build face index\n",
        "if face_embeddings is not None:\n",
        "    face_index = faiss.IndexFlatIP(512)\n",
        "    face_norm = face_embeddings / np.linalg.norm(face_embeddings, axis=1, keepdims=True)\n",
        "    face_index.add(face_norm.astype(np.float32))\n",
        "    print(f\"  âœ… Face index: {face_index.ntotal} vectors\")\n",
        "else:\n",
        "    face_index = faiss.IndexFlatIP(512)\n",
        "\n",
        "print(f\"\\nğŸ“Š Total: {image_index.ntotal} images, {face_index.ntotal} faces\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7B4ZHzXwPDg"
      },
      "source": [
        "## 4ï¸âƒ£ Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "996c358144aa4729886ccf5a20bcc122",
            "f356b92a247f4c278994071a32f5327f",
            "e16ad5b4733049bba6258707317fb8bc",
            "afdb0b85b1ed494baab3c6b483489505",
            "15af166fef014127985363ff809422b6",
            "5f486ea6c5734aa28bb47b694f68b712",
            "98ac580557ab425b94c051071f762a64",
            "b4c4e70082574763a567ae3edc4a0363",
            "0145c8f09c3f4328861534d21e5b4308",
            "e4dbb7d8adf541798a6a390aa3f8169c",
            "46e664817c244cd59bb5364440230052"
          ]
        },
        "id": "mpsCPEd5wPDg",
        "outputId": "ae4b3a58-99be-489a-e620-c3ca13f48661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading InsightFace...\n",
            "download_path: /root/.insightface/models/buffalo_l\n",
            "Downloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 281857/281857 [00:02<00:00, 110945.48KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n",
            "âœ… InsightFace loaded\n",
            "Loading CLIP ViT-L-14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "996c358144aa4729886ccf5a20bcc122"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CLIP loaded\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "âœ… YOLO loaded\n",
            "\n",
            "ğŸ‰ All models ready!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from insightface.app import FaceAnalysis\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# InsightFace\n",
        "print(\"Loading InsightFace...\")\n",
        "face_analyzer = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "face_analyzer.prepare(ctx_id=0, det_size=(640, 640))\n",
        "print(\"âœ… InsightFace loaded\")\n",
        "\n",
        "# CLIP ViT-L-14\n",
        "print(\"Loading CLIP ViT-L-14...\")\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai', device=DEVICE)\n",
        "clip_model.eval()\n",
        "clip_tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
        "print(\"âœ… CLIP loaded\")\n",
        "\n",
        "# YOLO (optional)\n",
        "yolo_model = None\n",
        "if os.path.exists(YOLO_MODEL_PATH):\n",
        "    from ultralytics import YOLO\n",
        "    yolo_model = YOLO(YOLO_MODEL_PATH)\n",
        "    print(\"âœ… YOLO loaded\")\n",
        "\n",
        "print(\"\\nğŸ‰ All models ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0LahWWTwPDi"
      },
      "source": [
        "## 5ï¸âƒ£ Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA8Fpl7FwPDi",
        "outputId": "72d1e7b8-ecfa-4b31-fff9-8ce8a556419d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Helper functions defined\n",
            "   Image paths available: 4998\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def get_image_path(idx):\n",
        "    \"\"\"Get image path from index.\"\"\"\n",
        "    if idx < len(image_paths_list):\n",
        "        return image_paths_list[idx]\n",
        "    return None\n",
        "\n",
        "def get_image_metadata(idx):\n",
        "    \"\"\"Get metadata for image index.\"\"\"\n",
        "    # Try different key formats\n",
        "    for key in [str(idx), idx, f\"image_{idx}\"]:\n",
        "        if key in image_metadata:\n",
        "            return image_metadata[key]\n",
        "    # Return basic info if no metadata found\n",
        "    path = get_image_path(idx)\n",
        "    return {\"path\": path, \"index\": idx} if path else {\"index\": idx}\n",
        "\n",
        "def embed_face(image_bytes):\n",
        "    \"\"\"Generate 512-dim face embedding.\"\"\"\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "    img_array = np.array(image)[:, :, ::-1]\n",
        "    faces = face_analyzer.get(img_array)\n",
        "    if not faces:\n",
        "        return None\n",
        "    best_face = max(faces, key=lambda f: f.det_score)\n",
        "    if best_face.det_score < 0.4:\n",
        "        return None\n",
        "    emb = best_face.embedding\n",
        "    return (emb / np.linalg.norm(emb)).astype(np.float32)\n",
        "\n",
        "def embed_image(image_bytes):\n",
        "    \"\"\"Generate 768-dim CLIP image embedding.\"\"\"\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "    img_input = clip_preprocess(image).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.encode_image(img_input)\n",
        "    emb = features.cpu().numpy().flatten()\n",
        "    return (emb / np.linalg.norm(emb)).astype(np.float32)\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"Generate 768-dim CLIP text embedding.\"\"\"\n",
        "    tokens = clip_tokenizer([text]).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.encode_text(tokens)\n",
        "    emb = features.cpu().numpy().flatten()\n",
        "    return (emb / np.linalg.norm(emb)).astype(np.float32)\n",
        "\n",
        "print(\"âœ… Helper functions defined\")\n",
        "print(f\"   Image paths available: {len(image_paths_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aga9Ho_HwPDl"
      },
      "source": [
        "## 6ï¸âƒ£ Search Functions (WITH Image Paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvcEePS8wPDl",
        "outputId": "8a0ba150-f86e-4523-c2d3-e403b6b1613f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Search functions ready (with image paths)\n"
          ]
        }
      ],
      "source": [
        "def search_by_face(image_bytes, top_k=10):\n",
        "    \"\"\"Search by face - returns results with image paths.\"\"\"\n",
        "    face_emb = embed_face(image_bytes)\n",
        "    if face_emb is None:\n",
        "        raise ValueError(\"No face detected\")\n",
        "\n",
        "    scores, indices = face_index.search(face_emb.reshape(1, -1), top_k)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx >= 0:\n",
        "            meta = get_image_metadata(int(idx))\n",
        "            img_path = get_image_path(int(idx))\n",
        "            results.append({\n",
        "                'index': int(idx),\n",
        "                'score': float(score),\n",
        "                'type': 'face',\n",
        "                'image_path': img_path,\n",
        "                'metadata': meta\n",
        "            })\n",
        "    return results\n",
        "\n",
        "\n",
        "def search_by_image(image_bytes, top_k=10):\n",
        "    \"\"\"Search by image - returns results with image paths.\"\"\"\n",
        "    image_emb = embed_image(image_bytes)\n",
        "\n",
        "    scores, indices = image_index.search(image_emb.reshape(1, -1), top_k)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx >= 0:\n",
        "            meta = get_image_metadata(int(idx))\n",
        "            img_path = get_image_path(int(idx))\n",
        "            results.append({\n",
        "                'index': int(idx),\n",
        "                'score': float(score),\n",
        "                'type': 'image',\n",
        "                'image_path': img_path,\n",
        "                'metadata': meta\n",
        "            })\n",
        "    return results\n",
        "\n",
        "\n",
        "def search_by_text(query, top_k=10):\n",
        "    \"\"\"Search by text - returns results with image paths.\"\"\"\n",
        "    text_emb = embed_text(query)\n",
        "\n",
        "    scores, indices = image_index.search(text_emb.reshape(1, -1), top_k)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx >= 0:\n",
        "            meta = get_image_metadata(int(idx))\n",
        "            img_path = get_image_path(int(idx))\n",
        "            results.append({\n",
        "                'index': int(idx),\n",
        "                'score': float(score),\n",
        "                'type': 'text',\n",
        "                'image_path': img_path,\n",
        "                'metadata': meta\n",
        "            })\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Search functions ready (with image paths)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNf7g87wPDp"
      },
      "source": [
        "## 7ï¸âƒ£ Test Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lgc2eGuwPDq",
        "outputId": "7ca53c8b-68a4-4604-f93e-279b27faeafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Results for 'person ':\n",
            "  1. Score: 0.220 | Path: /content/datasets/images/cifar100/00724_class66.png\n",
            "  2. Score: 0.218 | Path: /content/datasets/images/cifar100/00916_class4.png\n",
            "  3. Score: 0.217 | Path: /content/datasets/images/cifar100/02305_class64.png\n",
            "  4. Score: 0.216 | Path: /content/datasets/images/cifar100/03322_class32.png\n",
            "  5. Score: 0.215 | Path: /content/datasets/images/cifar100/03989_class66.png\n"
          ]
        }
      ],
      "source": [
        "# Test search\n",
        "if image_index.ntotal > 0:\n",
        "    results = search_by_text(\"Cat\", top_k=5)\n",
        "    print(\"ğŸ” Results for 'person ':\")\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"  {i}. Score: {r['score']:.3f} | Path: {r['image_path']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUT7-ocYwPDr"
      },
      "source": [
        "## 8ï¸âƒ£ FastAPI Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72TGayq9wPDs",
        "outputId": "3bcdf6c0-c39c-45a3-9423-d091a0754d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing server.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile server.py\n",
        "from fastapi import FastAPI, UploadFile, File, Form, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse, Response\n",
        "import time\n",
        "import os\n",
        "\n",
        "app = FastAPI(title=\"Multimodal Retrieval API\")\n",
        "\n",
        "# CORS middleware for all routes\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        "    expose_headers=[\"*\"]\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    from __main__ import image_index, face_index, image_paths_list\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"images_indexed\": image_index.ntotal,\n",
        "        \"faces_indexed\": face_index.ntotal,\n",
        "        \"image_paths_loaded\": len(image_paths_list)\n",
        "    }\n",
        "\n",
        "@app.post(\"/search/face\")\n",
        "async def api_search_face(file: UploadFile = File(...), top_k: int = Form(default=10)):\n",
        "    from __main__ import search_by_face\n",
        "    start = time.time()\n",
        "    contents = await file.read()\n",
        "    try:\n",
        "        results = search_by_face(contents, top_k)\n",
        "    except ValueError as e:\n",
        "        raise HTTPException(400, str(e))\n",
        "    return {\"query_type\": \"face\", \"results\": results, \"search_time_ms\": round((time.time()-start)*1000, 2)}\n",
        "\n",
        "@app.post(\"/search/image\")\n",
        "async def api_search_image(file: UploadFile = File(...), top_k: int = Form(default=10)):\n",
        "    from __main__ import search_by_image\n",
        "    start = time.time()\n",
        "    contents = await file.read()\n",
        "    results = search_by_image(contents, top_k)\n",
        "    return {\"query_type\": \"image\", \"results\": results, \"search_time_ms\": round((time.time()-start)*1000, 2)}\n",
        "\n",
        "@app.post(\"/search/text\")\n",
        "async def api_search_text(query: str = Form(...), top_k: int = Form(default=10)):\n",
        "    from __main__ import search_by_text\n",
        "    start = time.time()\n",
        "    results = search_by_text(query, top_k)\n",
        "    return {\"query_type\": \"text\", \"results\": results, \"search_time_ms\": round((time.time()-start)*1000, 2)}\n",
        "\n",
        "@app.get(\"/image/{idx}\")\n",
        "async def get_image(idx: int):\n",
        "    \"\"\"Serve image by index with CORS headers.\"\"\"\n",
        "    from __main__ import get_image_path\n",
        "    path = get_image_path(idx)\n",
        "    if path and os.path.exists(path):\n",
        "        # Read file and return with CORS headers\n",
        "        with open(path, \"rb\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Detect content type\n",
        "        ext = path.lower().split('.')[-1]\n",
        "        content_types = {\n",
        "            'jpg': 'image/jpeg',\n",
        "            'jpeg': 'image/jpeg',\n",
        "            'png': 'image/png',\n",
        "            'gif': 'image/gif',\n",
        "            'webp': 'image/webp'\n",
        "        }\n",
        "        content_type = content_types.get(ext, 'image/jpeg')\n",
        "\n",
        "        return Response(\n",
        "            content=content,\n",
        "            media_type=content_type,\n",
        "            headers={\n",
        "                \"Access-Control-Allow-Origin\": \"*\",\n",
        "                \"Cache-Control\": \"public, max-age=3600\"\n",
        "            }\n",
        "        )\n",
        "    raise HTTPException(404, \"Image not found\")\n",
        "\n",
        "@app.post(\"/embed/face\")\n",
        "async def api_embed_face(file: UploadFile = File(...)):\n",
        "    from __main__ import embed_face\n",
        "    contents = await file.read()\n",
        "    emb = embed_face(contents)\n",
        "    if emb is None:\n",
        "        raise HTTPException(400, \"No face detected\")\n",
        "    return {\"embedding\": emb.tolist(), \"dimension\": 512}\n",
        "\n",
        "@app.post(\"/embed/image\")\n",
        "async def api_embed_image(file: UploadFile = File(...)):\n",
        "    from __main__ import embed_image\n",
        "    contents = await file.read()\n",
        "    return {\"embedding\": embed_image(contents).tolist(), \"dimension\": 768}\n",
        "\n",
        "@app.post(\"/embed/text\")\n",
        "async def api_embed_text(query: str = Form(...)):\n",
        "    from __main__ import embed_text\n",
        "    return {\"embedding\": embed_text(query).tolist(), \"dimension\": 768}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvLQmQiMwPDv"
      },
      "source": [
        "## 9ï¸âƒ£ Run Server with ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeRjuAVnwPDv",
        "outputId": "744c0d86-4215-4a78-ae4b-546fbe288d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Set your ngrok auth token\n",
        "!ngrok authtoken 38FY2ZkXKAJzHfsWULq6acTAtpj_5sc8zM3Yg9BobtgdiCzbc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSqdL1RnwPDw",
        "outputId": "47abb82f-e14c-4302-d26e-608e3ca75884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸŒ PUBLIC URL: https://insensible-gregory-alphanumerically.ngrok-free.dev\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [5093]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Server starting...\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"POST /search/face HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/3343 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/4826 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/93 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/1395 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/374 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/7638 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/5405 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/7668 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/1878 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/1138 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/1595 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/5787 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/7872 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/1825 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/3798 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/7802 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/276 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/5215 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/248 HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:23ee:1698:3122:659a:23f6:d26e:75ae:0 - \"GET /image/1083 HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import asyncio\n",
        "\n",
        "# Start ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"\\nğŸŒ PUBLIC URL: {public_url.public_url}\")\n",
        "\n",
        "# IMPORTANT: Reload the server module\n",
        "import importlib\n",
        "import server\n",
        "importlib.reload(server)\n",
        "\n",
        "from server import app\n",
        "import uvicorn\n",
        "\n",
        "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server_instance = uvicorn.Server(config)\n",
        "\n",
        "print(\"\\nâœ… Server starting...\")\n",
        "await server_instance.serve()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "996c358144aa4729886ccf5a20bcc122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f356b92a247f4c278994071a32f5327f",
              "IPY_MODEL_e16ad5b4733049bba6258707317fb8bc",
              "IPY_MODEL_afdb0b85b1ed494baab3c6b483489505"
            ],
            "layout": "IPY_MODEL_15af166fef014127985363ff809422b6"
          }
        },
        "f356b92a247f4c278994071a32f5327f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f486ea6c5734aa28bb47b694f68b712",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_98ac580557ab425b94c051071f762a64",
            "value": "open_clip_model.safetensors:â€‡100%"
          }
        },
        "e16ad5b4733049bba6258707317fb8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c4e70082574763a567ae3edc4a0363",
            "max": 1710517724,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0145c8f09c3f4328861534d21e5b4308",
            "value": 1710517724
          }
        },
        "afdb0b85b1ed494baab3c6b483489505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4dbb7d8adf541798a6a390aa3f8169c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46e664817c244cd59bb5364440230052",
            "value": "â€‡1.71G/1.71Gâ€‡[00:02&lt;00:00,â€‡1.45GB/s]"
          }
        },
        "15af166fef014127985363ff809422b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f486ea6c5734aa28bb47b694f68b712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ac580557ab425b94c051071f762a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4c4e70082574763a567ae3edc4a0363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0145c8f09c3f4328861534d21e5b4308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4dbb7d8adf541798a6a390aa3f8169c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e664817c244cd59bb5364440230052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}